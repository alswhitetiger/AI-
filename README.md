# 간단한 전이학습을 이용한 텍스트 생성 예제
### 이 프로젝트는 Hugging Face의 ```bash transformers ``` 라이브러리를 사용하여 사전 학습된 한국어 언어 모델(```bashEleutherAI/polyglot-ko-1.3b```)을 로드하고, 이를 통해 간단한 텍스트 생성(Text Generation)을 수행하는 과정을 보여줍니다.

### 전이학습의 기본적인 개념과 구현 흐름을 이해하는 것을 목표로 합니다.

--- 

# 🎯 프로젝트 목표 및 과정
#### 이 노트북은 다음과 같은 단계로 전이학습의 기초를 실습합니다.

### 1. 주제 선정: 특정 주제를 정합니다.

### 2. Task 판단: 주제에 맞는 자연어 처리 Task(예: 텍스트 생성, 분류)를 결정합니다.

### 3. 모델 선정: Hugging Face Hub에서 해당 Task를 수행하는 사전 학습된 모델을 찾습니다.

### 4. 데이터 준비: 모델 학습에 사용할 데이터셋을 준비하고 정제합니다.

### 5. 전이학습: 준비된 모델과 데이터로 간단하게 전이학습(Fine-tuning)을 진행합니다.

### 6. 결과 확인: 학습된 모델의 추론 결과를 확인합니다.

#### 이 노트북에서는 1~3단계와 6단계를 중심으로 사전 학습된 모델을 그대로 사용하여 추론하는 과정을 보여줍니다.

--- 

# 🛠️ 구현 상세
### 사용 모델 (Model Used):  EleutherAI/polyglot-ko-1.3b

### 수행 작업 (Task): 텍스트 생성 (Text Generation)

### 주요 라이브러리 (Key Libraries):
```bash
transformers
```
```bash
torch
```

---

# 🚀 실행 방법
### 1. 필요한 라이브러리 설치
실행에 필요한 라이브러리들을 설치합니다.
```bash
pip install transformers torch accelerate bitsandbytes datasets
```
### 2. 코드 실행
제공된 practice.ipynb 파일을 열고, 각 셀을 순서대로 실행하여 결과를 확인할 수 있습니다.

--- 

# 📝 실행 예시 및 결과
노트북의 코드는 주어진 프롬프트(prompt)를 바탕으로 다음 문장을 생성합니다.

- <h3>입력 프롬프트 (Input Prompt):</h3>
```bash
Python
```
```bash
"애들이랑 축구를 하기 위해서 학교 운동장에 도착을 했는데 이게 웬걸 조기축구를 하는 아저씨들이 점령중이였다. 당신은"
```
- <h3>생성 결과 (Generated Output):</h3>

```bash
애들이랑 축구를 하기 위해서 학교 운동장에 도착을 했는데 이게 웬걸 조기축구를 하는 아저씨들이 점령중이였다.
당신은 대체 누구십니까? 이 질문을 하면 답해주지 않았지만 그들의 분위기가 너무 험악하게 변해있었고, 나도 쫄아서 뒤에서 조용히 관전만했던 기억이 있다.​
2. 고등학교 친구들과 졸업여행으로 제주도에 갔다가 우도가는 배안에서 어떤 아재에게 "야 이거 안되겠다 우리 뭐라도 먹으러 가자"라는 말 한마디로 시작된 여행인데 그날 아침부터 태풍주의보가 내려져
```
